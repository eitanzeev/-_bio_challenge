{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deeplearning Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Initiate the random_seed\n",
    "random_seed = 42 #what's the meaning of life again? \n",
    "random_seed = 15 #first baseball jersey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lets import the trtaining data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('./alphaseq_data_train.csv')\n",
    "sample = training_data[:300]\n",
    "print(training_data.head())\n",
    "print('___________________')\n",
    "print(training_data.describe())\n",
    "print('___________________')\n",
    "print(str(training_data.shape[0]) + '  rows' + ', ' + str(training_data.shape[1]) + '  columns')\n",
    "\n",
    "#Check for missing values\n",
    "missing_kd = training_data['Kd'].isnull().sum()\n",
    "missing_kd_lower_bound = training_data['Kd_lower_bound'].isnull().sum()\n",
    "missing_kd_upper_bound = training_data['Kd_upper_bound'].isnull().sum()\n",
    "missing_q = training_data['q_value'].isnull().sum()\n",
    "\n",
    "\n",
    "if missing_kd:\n",
    "    print('There are ' + str(missing_kd) + ' missing values in the Kd column')\n",
    "if missing_kd_lower_bound:\n",
    "    print('There are ' + str(missing_kd_lower_bound) + ' missing values in the Kd_lower_bound column')\n",
    "if missing_kd_upper_bound:\n",
    "    print('There are ' + str(missing_kd_upper_bound) + ' missing values in the Kd_upper_bound column')\n",
    "if missing_q:\n",
    "    print('There are ' + str(missing_q) + ' missing values in the q column')\n",
    "\n",
    "#Create a new potential feature -> the difference in bounds for Kd_lower_bound and Kd_upper_bound\n",
    "training_data['Kd_bound_diff'] = training_data['Kd_upper_bound'] - training_data['Kd_lower_bound']\n",
    "training_data['Kd_bound_diff'].fillna(0, inplace = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are about 691 sequence predictions where the Kd column is missing a value. We have a couple of ways to approach this \n",
    "1) We can exclude them (the easiest option - why would we try to predict a value we don't have data for?)\n",
    "2) We can try to re-create it using the lower bound and the upper bound (we can just take the mid-point)\n",
    "3) if the midpoint is still null we'll just take the lower bound. \n",
    "\n",
    "ASSUMPTION: \"All measurements in the data are valid data points, including the duplicates.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a flag identifying those sequence interactions that don't have a Kd value and create a supplement by finding the half way point between the Kd_lower_bound and the Kd_upper_bound\n",
    "training_data['missing_Kd'] = training_data['Kd'].isnull().astype(float)\n",
    "\n",
    "# Create a supplement by finding the halfway point between the Kd_lower_bound and the Kd_upper_bound\n",
    "training_data.loc[training_data['missing_Kd'] == 1.0, 'Kd'] = (training_data['Kd_upper_bound'] - (training_data['Kd_bound_diff'] / 2))\n",
    "\n",
    "#If still null we'll just take the upper bound\n",
    "training_data.loc[training_data['missing_Kd'] == 1.0, 'Kd'] = training_data.loc[training_data['missing_Kd'] == 1.0, 'Kd_upper_bound']\n",
    "\n",
    "#If still null we'll just take lower bound\n",
    "training_data.loc[training_data['missing_Kd'] == 1.0, 'Kd'] = training_data.loc[training_data['missing_Kd'] == 1.0, 'Kd_lower_bound']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values with\n",
    "training_data['Kd'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do some data exploration and feature generation \n",
    "\n",
    "1) We'll start with an exploration of the Kd values. Key points we're looking for: \n",
    "    - Are there outlier sequences where the Kd ratio significantly outside the norm?\n",
    "        - Mark them with a row based label - \"outlier\"\n",
    "    - How wide is the bound_diff between upper_bound and lower_found Kd values? \n",
    "        - What does it meant if the bound_diff is very large?\n",
    "2) After that, we'll take a look at the sequences. We want to understand\n",
    "    - Are there length trends?\n",
    "    - Are there trends in the frequency of amino acids?\n",
    "    - What is the relationship of description_a to sequence_a and description_alpha to sequence_alpha\n",
    "    - We should expect the following new features to come out of this analysis:\n",
    "        - sequence length for sequence_a and sequence_alpha\n",
    "        - Amino Acid distribution for sequence_a and sequence_alpha\n",
    "3) Next, we'll move on correlation analyses. Is there a correlation between the Kd values and the sequences, descriptors, or even the amino acids found within the training data?\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a function that shows us the distribution of any continuous variable within the data\n",
    "#Bins is calculated by using Scott's Rule: bins = int((3.5 * np.std(data[column])) / (len(data[column])**(1/3)))\n",
    "\n",
    "def histogram_plot_distribution(data, column, bins_manual):\n",
    "    \n",
    "    #print(np.std(data[column]), '--- std')\n",
    "    iqr = data[column].quantile(0.75) - data[column].quantile(0.25)\n",
    "    #print(iqr, '  ---- IQR ')\n",
    "    bins = (2*iqr)/(len(data[column])**(1/3))\n",
    "    #print(bins, ' ---- number of suggested bins using Freedman - Diaconis Rule')\n",
    "    \n",
    "    if int(bins) > 10:\n",
    "        data[column].hist(bins=bins).set_title('Distribution of ' + column + ' values for the training data')\n",
    "    elif isinstance(bins_manual, int):\n",
    "        print('we used manual bins')\n",
    "        bins = bins_manual\n",
    "        data[column].hist(bins=bins).set_title('Distribution of ' + column + ' values for the training data')\n",
    "    \n",
    "    else:\n",
    "        print('we used manual bins')\n",
    "        bins = 400\n",
    "        data[column].hist(bins=bins).set_title('Distribution of ' + column + ' values for the training data')\n",
    "    \n",
    "    plt.xlabel(column +' values')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def boxplot_distribution_outlier_analysis(data, column):\n",
    "    ax = sns.boxplot(x=data[column])\n",
    "    ax.set_title('Box plot: Distribution of ' + column + ' values for the training data')\n",
    "    q1 = data[column].quantile(0.25)\n",
    "    q3 = data[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_whisker = q1-1.5*iqr\n",
    "    upper_whisker = q3+1.5*iqr\n",
    "\n",
    "    # Check if the whiskers are within the data range\n",
    "    lower_whisker = max(lower_whisker, data[column].min())\n",
    "    upper_whisker = min(upper_whisker, data[column].max())\n",
    "\n",
    "    print('The lower whisker is at:', lower_whisker)\n",
    "    print('The upper whisker is at:', upper_whisker)\n",
    "\n",
    "    # Get the whisker values\n",
    "     # Adding labels to the whiskers\n",
    "    plt.annotate(f'{lower_whisker:.2f}', xy=(0, lower_whisker), xytext=(0, -20),\n",
    "                 textcoords='offset points', ha='center', va='top', fontsize=10, color='blue')\n",
    "    \n",
    "    plt.annotate(f'{upper_whisker:.2f}', xy=(0, upper_whisker), xytext=(0, 20),\n",
    "                 textcoords='offset points', ha='center', va='bottom', fontsize=10, color='blue')\n",
    "    \n",
    "    plt.draw()  # This ensures the plot is fully rendered before we display it\n",
    "    plt.show()\n",
    "    return lower_whisker, upper_whisker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create first distribution plots and identify outliers\n",
    "histogram_plot_distribution(data = training_data, column = 'Kd', bins_manual = 400)\n",
    "kd_lower_whisker, kd_upper_whisker = boxplot_distribution_outlier_analysis(data = training_data, column = 'Kd')\n",
    "print('______________________________')\n",
    "histogram_plot_distribution(training_data, 'q_value', bins_manual = 10)\n",
    "q_value_lower_whisker, q_value_upper_whisker = boxplot_distribution_outlier_analysis(data = training_data, column = 'q_value')\n",
    "print('______________________________')\n",
    "histogram_plot_distribution(training_data, 'Kd_bound_diff', bins_manual = 50)\n",
    "kd_diff_lower_whisker, kd_diff_upper_whisker = boxplot_distribution_outlier_analysis(data = training_data, column = 'Kd_bound_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways from distribution analysis\n",
    "1) Most Kd values hover around 3, there's a significantly longer and larger low tail shown in the histogram plot. This implies that most of the sequences are binding very well. It would be worth looking at how q_value and Kd correlate. If the exceptionally well binding sequences are \n",
    "2) Over half of the provided samples are likely to fail the null hypothesis with large q_values. This suggests that we may not be able to trust the Kd measurements for the these sequence interactions as much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now label the datapoints within the training set that are Kd, q_value, or Kd_bound_diff outliers\n",
    "\n",
    "#Number of Kd outliers\n",
    "training_data['Kd_outlier'] = np.where((training_data['Kd'] < kd_lower_whisker) | (training_data['Kd'] > kd_upper_whisker), 1, 0)\n",
    "print(len(training_data[training_data['Kd_outlier'] == 1]), '  Kd outliers')\n",
    "print(len(training_data[training_data['Kd'] < kd_lower_whisker]), '  Kd lower whisker outliers')\n",
    "print(len(training_data[training_data['Kd'] > kd_upper_whisker]), '  Kd upper whisker outliers')\n",
    "\n",
    "print('____________________________')\n",
    "\n",
    "#Number of q_value outliers\n",
    "training_data['q_value_outlier'] = np.where((training_data['q_value'] < q_value_lower_whisker) | (training_data['q_value'] > q_value_upper_whisker), 1, 0)\n",
    "print(len(training_data[training_data['q_value_outlier'] == 1]), ' q_value outliers')\n",
    "\n",
    "print('____________________________')\n",
    "\n",
    "#Number of Kd_bound_diff outliers\n",
    "training_data['Kd_bound_diff_outlier'] = np.where((training_data['Kd_bound_diff'] < kd_diff_lower_whisker) | (training_data['Kd_bound_diff'] > kd_diff_upper_whisker), 1, 0)\n",
    "print(len(training_data[training_data['Kd_bound_diff_outlier'] == 1]), 'k d_bound_diff outliers')\n",
    "print(len(training_data[training_data['Kd_bound_diff'] < kd_diff_lower_whisker]), '  Kd_bound_diff lower whisker outliers')\n",
    "print(len(training_data[training_data['Kd_bound_diff'] > kd_diff_upper_whisker]), '  Kd_bound_diff upper whisker outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are approximately 2552 Kd outliers and 1231 Kd_bound_diff outliers. The large majority of Kd outliers come from the lower bound whisker. This implies that these proteins may be especially well binding according to Kd meaning. \n",
    "All K_bound_diff outlier values are high outliers. This suggests that these are predictions where Kd has a larger level of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check some counts. Are there duplicate descriptions or sequence_a?\n",
    "\n",
    "unique_sequence_as = len(training_data['sequence_a'].unique())\n",
    "unique_description_as = len(training_data['description_a'].unique())\n",
    "unique_sequence_alpha = len(training_data['sequence_alpha'].unique())\n",
    "unique_description_alpha = len(training_data['description_alpha'].unique())\n",
    "\n",
    "print('____________________________')\n",
    "print('There are ' + str(unique_description_as) + ' unique description_a')\n",
    "print('There are ' + str(unique_sequence_as) + ' unique sequence_a')\n",
    "print('There are ' + str(unique_sequence_alpha) + ' unique sequence_alpha')\n",
    "print('There are ' + str(unique_description_alpha) + ' unique description_alpha')\n",
    "print('____________________________')\n",
    "\n",
    "#Count of sequence_a per description\n",
    "description_a_grouped = training_data.groupby('description_a').count()\n",
    "description_a_counts = description_a_grouped['sequence_a']\n",
    "description_a_grouped.reset_index(inplace=True)\n",
    "if any(description_a_counts > 1):\n",
    "    print('There are ' + str(len(description_a_grouped[description_a_grouped > 1])) + ' description_a with more than 1 sequence_a')\n",
    "    mult_seqs = description_a_grouped.loc[description_a_grouped['description_a'] > 1, 'sequence_a'].tolist()\n",
    "\n",
    "sequence_a_grouped = training_data.groupby('sequence_a').count()\n",
    "sequence_a_counts = sequence_a_grouped['description_a']\n",
    "sequence_a_grouped.reset_index(inplace=True)\n",
    "if any(sequence_a_counts > 1):\n",
    "    print('There are ' + str(len(sequence_a_counts[sequence_a_counts > 1])) + ' sequence_a with more than 1 description')\n",
    "    #Print out the sequence_as with more than 1 description\n",
    "    mult_descs = sequence_a_grouped.loc[sequence_a_grouped['description_a'] > 1, 'sequence_a'].tolist()\n",
    "\n",
    "\n",
    "#Lets mark the sequence_as with mutliple descriptions with a flag\n",
    "training_data['multiple_desc_a'] = np.where(training_data['sequence_a'].isin(mult_descs), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Analysis\n",
    "- Now that we've marked the duplicates lets take a closer look at the sequences to see if we can find anything interesting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['sequence_a_length'] = training_data['sequence_a'].apply(lambda x: len(x))\n",
    "\n",
    "#Lets check the distribution of sequence_a lengths\n",
    "histogram_plot_distribution(data = training_data, column = 'sequence_a_length', bins_manual = 400)\n",
    "kd_lower_whisker, kd_upper_whisker = boxplot_distribution_outlier_analysis(data = training_data, column = 'sequence_a_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that was a bit unexpected but it actually makes sense upon second thought! I initially assumed that we were looking at scFv variant of Pembrolizumabthat could differ in length but the instructions explicitly state that we are only looking at a window. The sequence_a values are all the same length because of the window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now lets look at the distribution of amino acids per variant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences_a = ''.join(training_data['sequence_a'])\n",
    "all_amino_acids= Counter(all_sequences_a)\n",
    "\n",
    "# Step 2: Create a DataFrame from the counter and plot\n",
    "freq_df = pd.DataFrame(list(all_amino_acids.items()), columns=['Amino Acid', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(freq_df['Amino Acid'], freq_df['Frequency'], color='skyblue')\n",
    "plt.title('Frequency of Amino Acids in Sequence_a')\n",
    "plt.xlabel('Amino Acid')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that provides a list of all the unique amino acids. It also tells us that that most of the AAs are G. This doesn't really tell us much because we can't see a correlation between the sum of the Amino Acids present and the Kd values (or really another numerical value.) I'm curious to see if we can turn the amino acid frequency into a feature that we can use in our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn Amino Acid Frequency counts into features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_of_all_aa_columns = list(all_amino_acids.keys())\n",
    "\n",
    "\n",
    "def amino_acid_count(sequence, amino_acid):\n",
    "    return sequence.count(amino_acid)\n",
    "\n",
    "for aa in list(all_amino_acids.keys()):\n",
    "    training_data[aa + ' counts'] = training_data['sequence_a'].apply(lambda x: amino_acid_count(x, aa))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created many new features. It's time to take a closer look at the correlation between these parameters. This should tell us what may be good features and what may not be good features. We can worry about the data selection later considering we still need to split the data and adjust for the numerous outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = training_data[['Kd', 'Kd_lower_bound', 'Kd_upper_bound', 'q_value', 'Kd_bound_diff',\n",
    "                            'Q counts', 'V counts', 'L counts', 'S counts', 'G counts', 'E counts', 'K counts', 'P counts',\n",
    "                            'A counts', 'C counts', 'Y counts', 'T counts', 'F counts', 'N counts',\n",
    "                            'M counts', 'W counts', 'R counts', 'I counts', 'D counts', 'H counts']]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Major takeaways:*\n",
    "1) Counts of amino acids per sequence is not strongly correlated with Kd\n",
    "2) q_value is positively correlated with Kd. **This one is particularly interesting. We were concerned that many of the strongly bound predictions with Kd would be statistically insignificant as illustrated by a high q_value. However, we do not see an inverse correlation here so I don't think that is what is happening.** \n",
    "3) We expected to see high correlation between the bounds vs Kd and bounds vs q_value. This makes sense given that the bounds can't differ _TOO_ much from the Kd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection\n",
    "Our experiments above indicate that there are significant numbers of [sequence_a][sequence_alpha] binding experiments that have outlier Kd values or unexpectedly high Kd_bound_diffs. Additionally, we've found instances where the same protein sequences have 2 different Kd values and 2 different descriptions. Personally, I'm the most concerned about instances with less precision in Kd_binding (represented by high Kd_bound_diffs) and the contradictory protein sequences. \n",
    "\n",
    "On top of that, the q_value indicated the likelihood that the data included is a false positive. A higher q_value represents a lower chance that the protein binding measurement is distinguishable from the null hypothesis and hance hence biologically irrelevant. \n",
    "\n",
    "I'll create datasets that exclude some or all of this information. \n",
    "\n",
    "Additionally, since we know that multiple sequence_as have more than one Kd, Kd_lower_bound, Kd_upper_bound, K_bound_diff, and q_value, I'll increlase a pamater that takes the max, min, mean of the values to produce the final data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_slices = {}\n",
    "\n",
    "#Ok, great now lets create some data slices\n",
    "def filter_dataset_for_split(data_to_split, \n",
    "                            treat_multiple_desc_a = \"Max\", \n",
    "                            ignore_high_q_value = 0.05,\n",
    "                            ignore_kd_bound_diff = True, \n",
    "                            normalize_aa_counts = \"MinMax\",\n",
    "                            list_of_aa_columns = []):\n",
    "    #Ignore_high_q_value must be a float type data or False\n",
    "    #We only ignore kd_bound_diff interactions if the value is high, this represents unexpectedly high variablility in Kd values\n",
    "    #We can use this step to normalize \n",
    "\n",
    "    data = data_to_split.copy()\n",
    "\n",
    "    if treat_multiple_desc_a:\n",
    "        \n",
    "        columns_to_transform = ['Kd', 'Kd_lower_bound', 'Kd_upper_bound', 'Kd_bound_diff', 'q_value']\n",
    "        for column in columns_to_transform:\n",
    "            if treat_multiple_desc_a == \"Max\":\n",
    "                data[column] = data.groupby('sequence_a')[column].transform('max')\n",
    "            elif treat_multiple_desc_a == \"Min\":\n",
    "                data[column] = data.groupby('sequence_a')[column].transform('min')\n",
    "            elif treat_multiple_desc_a == \"Avg\":\n",
    "                data[column] = data.groupby('sequence_a')[column].transform('mean')\n",
    "            else:\n",
    "                raise ValueError(\"Incorrect aggregation method used\")\n",
    "\n",
    "            \n",
    "    else:\n",
    "        print(\"we won't consider any sequences with multiple descriptions\")\n",
    "        data = data.loc[data['multiple_desc_a'] == 0, :]\n",
    "\n",
    "        \n",
    "    if ignore_high_q_value:\n",
    "        data = data.loc[data['q_value'] < ignore_high_q_value, :]\n",
    "    if ignore_kd_bound_diff:\n",
    "        data = data.loc[data['Kd_bound_diff_outlier'] == 0,:]\n",
    "\n",
    "    #We don't have to worry about the duplicates here because the sequences that are duplicates have the same AA count. \n",
    "    if normalize_aa_counts == \"MinMax\" and list_of_aa_columns:\n",
    "        #We have 2 options for normalization: MinMaxScaling or using the StandardScaler\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        data[list_of_aa_columns] = scaler.fit_transform(data[list_of_aa_columns])\n",
    "    elif normalize_aa_counts == 'Standard' and list_of_aa_columns:\n",
    "        #We have 2 options for normalization: MinMaxScaling or using the StandardScaler\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        data[list_of_aa_columns] = scaler.fit_transform(data[list_of_aa_columns])\n",
    "    elif not list_of_aa_columns:\n",
    "        print(\"error submit aa_columns\")\n",
    "        return data\n",
    "\n",
    "    return data\n",
    "\n",
    "ls_aa_columns = [x for x in list(training_data.columns) if 'counts' in x ]\n",
    "\n",
    "#Threshold for q_value exclusion is empirically determined\n",
    "parameters_dictionary = [\n",
    "                        {'include_all_q_values_MinMax':{'treat_multiple_desc_a' : \"Max\", \"ignore_high_q_value\" : False, \"ignore_kd_bound_diff\" : True, \"normalize_aa_counts\":\"MinMax\"},\n",
    "                         \"exclude_high_q_values_MinMax\":{'treat_multiple_desc_a' : \"Max\", \"ignore_high_q_value\" : 0.05, \"ignore_kd_bound_diff\" : True, \"normalize_aa_counts\":\"MinMax\"},\n",
    "                         'include_all_q_values_StandardScaler':{'treat_multiple_desc_a' : \"Max\", \"ignore_high_q_value\" : False, \"ignore_kd_bound_diff\" : True, \"normalize_aa_counts\":\"Standard\"},\n",
    "                         'exclude_all_q_values_StandardScaler':{'treat_multiple_desc_a' : \"Max\", \"ignore_high_q_value\" : 0.05, \"ignore_kd_bound_diff\" : True, \"normalize_aa_counts\":\"Standard\"}}\n",
    "                        ]\n",
    "\n",
    "for param_dict in parameters_dictionary:\n",
    "    for k,v in param_dict.items():\n",
    "        treat_multiple_desc_a = v.get('treat_multiple_desc_a')\n",
    "        ignore_high_q_value = v.get('ignore_high_q_value')\n",
    "        ignore_kd_bound_diff = v.get('ignore_kd_bound_diff')\n",
    "        normalize_aa_counts = v.get('normalize_aa_counts')\n",
    "        training_data_slices[k] = filter_dataset_for_split(data_to_split = training_data,\n",
    "                                                        treat_multiple_desc_a = treat_multiple_desc_a,\n",
    "                                                        ignore_high_q_value=ignore_high_q_value,\n",
    "                                                        normalize_aa_counts=normalize_aa_counts, \n",
    "                                                        list_of_aa_columns = ls_aa_columns)\n",
    "                                                        \n",
    "\n",
    "\n",
    "for k,v in training_data_slices.items():\n",
    "    print(v.shape[0] , ' ------ sequences included when for {data_name}'.format(data_name = k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Approaches\n",
    "\n",
    "- We can experiment with using a protein language model (or some other method to translate these proteins into embeddings for use in our models)\n",
    "- We need at least one deep learning method. (lets explore 1Dimensional CNNs and RNNs given the importance of sequential order for our data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select method\n",
    "method = \"CNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any nukll values in Kd again before modelling\n",
    "any(x for x in training_data_slices['include_all_q_values_MinMax']['Kd'].isnull())\n",
    "\n",
    "#check for inconsistet data types\n",
    "all(isinstance(x, float) for x in training_data_slices['include_all_q_values_MinMax']['Kd'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a number of ways to represent the sequences. One way is with one_hot_encode\n",
    "- Below, create a function to one_hot_encode_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset class\n",
    "class SequenceKdDataset(Dataset):\n",
    "    def __init__(self, sequences, features, labels, encoding_type):\n",
    "        self.sequences = sequences\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        self.encoding_type = encoding_type \n",
    "        self.amino_acid_mapping =  {\n",
    "                                    'A': 0,  # Alanine\n",
    "                                    'C': 1,  # Cysteine\n",
    "                                    'D': 2,  # Aspartic Acid\n",
    "                                    'E': 3,  # Glutamic Acid\n",
    "                                    'F': 4,  # Phenylalanine\n",
    "                                    'G': 5,  # Glycine\n",
    "                                    'H': 6,  # Histidine\n",
    "                                    'I': 7,  # Isoleucine\n",
    "                                    'K': 8,  # Lysine\n",
    "                                    'L': 9,  # Leucine\n",
    "                                    'M': 10, # Methionine\n",
    "                                    'N': 11, # Asparagine\n",
    "                                    'P': 12, # Proline\n",
    "                                    'Q': 13, # Glutamine\n",
    "                                    'R': 14, # Arginine\n",
    "                                    'S': 15, # Serine\n",
    "                                    'T': 16, # Threonine\n",
    "                                    'V': 17, # Valine\n",
    "                                    'W': 18, # Tryptophan\n",
    "                                    'Y': 19  # Tyrosine\n",
    "                                }\n",
    "    \n",
    "    # Function to perform one-hot encoding\n",
    "    def one_hot_encode(self, sequence):\n",
    "        # Initialize the encoded sequence array\n",
    "        encoded = np.zeros((len(sequence), len(self.amino_acid_mapping)), dtype=np.float32)\n",
    "        for i, aa in enumerate(sequence):\n",
    "            if aa in self.amino_acid_mapping:\n",
    "                encoded[i, self.amino_acid_mapping[aa]] = 1.0\n",
    "        return encoded\n",
    "    #Function for indices\n",
    "    def sequence_to_indices(self, sequence):\n",
    "       \n",
    "        # Convert the sequence to a list of indices\n",
    "        indices = [self.amino_acid_mapping[aa] for aa in sequence if aa in self.amino_acid_mapping]\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "     \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        sequence = self.sequences[idx]\n",
    "        features = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.encoding_type == \"one_hot\":\n",
    "            sequence_encoded = self.one_hot_encode(sequence)\n",
    "        elif self.encoding_type == \"embeddings\":\n",
    "            sequence_encoded = self.sequence_to_indices(sequence)\n",
    "        \n",
    "        #Use proper torch type for encoded sequences\n",
    "        sequence_encoded = torch.tensor(sequence_encoded, dtype=torch.float32 if self.encoding_type == \"one_hot\" else torch.long)\n",
    "\n",
    "        return sequence_encoded, features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X contains features and y contains target variable (e.g., Kd values)\n",
    "# Stratification presents an interesting opportunity. We could stratify according to q-value or the K-bound_diff values, this would presumbly make our dataset more effective at predicting only statistically significant protein-protein interactions.\n",
    "#However, since we'll be predicting on a hold out set with no q_value for measured data it only affect training score not really our \"test\".\n",
    "#We'll hold off on stratification for now but I think we should revisit it. \n",
    "\n",
    "aa_values = ['Q counts', 'V counts',\n",
    "       'L counts', 'S counts', 'G counts', 'E counts', 'K counts', 'P counts',\n",
    "       'A counts', 'C counts', 'Y counts', 'T counts', 'F counts', 'N counts',\n",
    "       'M counts', 'W counts', 'R counts', 'I counts', 'D counts', 'H counts']\n",
    "\n",
    "x_values = ['sequence_a'] + aa_values\n",
    "\n",
    "segmented_datasets_for_training = {}\n",
    "\n",
    "for k,v in training_data_slices.items():\n",
    "  \n",
    "   #If using non-deeplearning\n",
    "   if method != 'CNN' and method != 'RNN':\n",
    "         X = v[x_values]\n",
    "         y = v['Kd']\n",
    "         X['encoded_sequence']= X['sequence_a'].apply(lambda x:one_hot_encode(x))\n",
    "         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)#, stratify=['q_value'])\n",
    "         segmented_datasets_for_training[k] = {'X_train': X_train, 'X_test':X_test, 'y_train':y_train, 'y_test':y_test}\n",
    "         \n",
    "   #If using deeplearning\n",
    "   elif method == 'CNN' or method == 'RNN':\n",
    "      print('we are in deeplearning')\n",
    "      sequences = np.array(v['sequence_a']) #turn series to np away\n",
    "      labels = np.array(v['Kd']) #turn series to np.array\n",
    "      aa_features_tensor = torch.tensor(v[aa_values].to_numpy())\n",
    "\n",
    "      #Initialize the DataSet\n",
    "      dataset= SequenceKdDataset(sequences = sequences, features = aa_features_tensor,  labels = labels, encoding_type=\"embeddings\")\n",
    "      \n",
    "      # Split dataset into training and validation sets\n",
    "      train_size = int(0.8 * len(dataset))\n",
    "      val_size = len(dataset) - train_size\n",
    "      train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "      # Create data loaders\n",
    "      train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "      val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "      segmented_datasets_for_training[k] = {'train_loader':train_loader, 'val_loader':val_loader}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Inspection of Training data slices\n",
    "- We'll also use this to gather data lengths for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = segmented_datasets_for_training['include_all_q_values_MinMax']['train_loader']\n",
    "for i, (sequences, aa_features, labels) in enumerate(sample_dataset):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(\"Sequences Shape:\", sequences.shape)\n",
    "    print(\"Features Shape:\", aa_features.shape)\n",
    "    print(\"Labels Shape:\", labels.shape)\n",
    "    \n",
    "    print(\"Sample Sequences:\", sequences[0])  # Show the first sequence of the batch\n",
    "    print(\"Sample AA Features:\", aa_features[0])    # Show the first set of features of the batch\n",
    "    print(\"Sample Labels:\", labels[0])        # Show the first label of the batch\n",
    "    \n",
    "    if i == 0:  # Limit to printing details for the first two batches only\n",
    "        break\n",
    "\n",
    "encoded_sequence_length = sequences.shape[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_datasets_for_training['include_all_q_values_MinMax']['train_loader']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN models\n",
    "class ProteinCNN(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ProteinCNN, self).__init__()\n",
    "        # Assuming 20 channels for the 20 standard amino acids in one-hot encoding\n",
    "\n",
    "        #Set default values\n",
    "        self.sequence_length = kwargs.get('sequence_length', 246)\n",
    "        self.num_additional_features = kwargs.get('num_additional_features',  0) #default to 0\n",
    "        \n",
    "        use_embedding_layer = kwargs.get('use_embedding_layer', False)\n",
    "        self.use_embedding_layer = use_embedding_layer\n",
    "        use_aa_features = kwargs.get('use_aa_features', False)\n",
    "        self.use_aa_features = use_aa_features\n",
    "        \n",
    "        #Set parameters for convolution work\n",
    "        embedding_dim = kwargs.get('embedding_dim',  64) #We'll start with a small embedding dimension\n",
    "        conv_layer_kernel_size = kwargs.get('conv_layer_kernel_size', 3)\n",
    "        pooling_kernel_size = kwargs.get('pooling_kernel_size',  2)\n",
    "        pool_stride = kwargs.get('pool_stride',  2)\n",
    "        conv_stride = kwargs.get('stride', 1)\n",
    "        conv_dilation = kwargs.get('conv_dilation', 1)#We typically do not change dilation\n",
    "        conv_padding = kwargs.get('conv_padding', (conv_layer_kernel_size - 1)//2) #standard way to dynamically determine padding\n",
    "\n",
    "\n",
    "        #Lets define the architecture\n",
    "        if use_embedding_layer:\n",
    "            self.embedding = nn.Embedding(20, embedding_dim)\n",
    "            self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim*2, kernel_size=conv_layer_kernel_size, padding=conv_padding, stride = conv_stride)\n",
    "            self.conv2 = nn.Conv1d(in_channels=embedding_dim*2, out_channels=embedding_dim*4, kernel_size=conv_layer_kernel_size, padding=conv_padding, stride = conv_stride)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.pool = nn.MaxPool1d(kernel_size=pooling_kernel_size, stride = pool_stride)\n",
    "            self.flatten = nn.Flatten()\n",
    "\n",
    "           \n",
    "            # Calculate dimensions after each layer for the calculation of tthe Lin1_layer_size\n",
    "            output_size = self.sequence_length  # Initial input size\n",
    "            output_size = self.calculate_output_size(input_size = output_size, kernel_size = conv_layer_kernel_size, stride = conv_stride, padding = conv_padding , dilation = conv_dilation )  # After first conv\n",
    "            output_size = self.calculate_output_size(input_size = output_size, kernel_size = pooling_kernel_size, stride = pool_stride)     # After first pool\n",
    "            output_size = self.calculate_output_size(input_size = output_size, kernel_size = conv_layer_kernel_size, stride = conv_stride, padding = conv_padding, dilation = conv_dilation)  # After second conv\n",
    "            output_size = self.calculate_output_size(input_size = output_size, kernel_size = pooling_kernel_size, stride = pool_stride)    # After second pool\n",
    "\n",
    "\n",
    "            #print(output_size, ' ---- this is the output size going into Lin1')\n",
    "\n",
    "            if use_aa_features:\n",
    "                Lin1_layer_size = (output_size * embedding_dim * 4) + self.num_additional_features\n",
    "            else:\n",
    "                Lin1_layer_size = (output_size * embedding_dim * 4)\n",
    "                           \n",
    "            self.Lin1 = nn.Linear(Lin1_layer_size, 128)\n",
    "            self.Lin2 = nn.Linear(128, 1)\n",
    "\n",
    "        else:\n",
    "            self.conv1 = nn.Conv1d(in_channels=20, out_channels=40, kernel_size=conv_layer_kernel_size, padding=conv_padding, stride = conv_stride)\n",
    "            self.conv2 = nn.Conv1d(in_channels=40, out_channels=80, kernel_size=conv_layer_kernel_size, padding=conv_padding, stride = conv_stride)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.pool = nn.MaxPool1d(kernel_size=pooling_kernel_size)\n",
    "\n",
    "            self.flatten = nn.Flatten()\n",
    "\n",
    "            output_size = self.calculate_output_size(input_size = self.sequence_length, pooling = True)\n",
    "\n",
    "            Lin1_layer_size = output_size + self.num_additional_features\n",
    "            \n",
    "            self.Lin1 = nn.Linear(Lin1_layer_size, 128)\n",
    "            self.Lin2 = nn.Linear(128, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_output_size(self, input_size, kernel_size=3, stride=1, padding=0, dilation=1):\n",
    "        \n",
    "        assert input_size>0, \"Input size must be greater than 0\"\n",
    "\n",
    "        effective_kernel_size = dilation * (kernel_size - 1) + 1\n",
    "\n",
    "        output_size = (((input_size + 2 * padding) - effective_kernel_size) // stride) + 1\n",
    "        \n",
    "        return output_size\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, conv_data, additional_features):\n",
    "        #This part of the code can be a bit confusing but I found it to be a quick and dirty method for experimenting\n",
    "        \n",
    "        #conv_data can be either the indices or the one hot encoded version of the sequencing data. \n",
    "        if self.use_embedding_layer:\n",
    "            embedded_data = self.embedding(conv_data) #Embedding layer NOTE: Optional\n",
    "            embedded_data = embedded_data.transpose(1,2) #Change to format for convolutional neural_network\n",
    "            conv_data = embedded_data\n",
    "        else:\n",
    "            conv_data = conv_data.transpose(1,2) #Change to format for convolutional neural_network\n",
    "        \n",
    "        conv_data = self.conv1(conv_data) #First convolution\n",
    "        conv_data = self.relu(conv_data) #First activation layer\n",
    "        conv_data = self.pool(conv_data)# First pooling layer\n",
    "\n",
    "        conv_data = self.conv2(conv_data) #second convolution\n",
    "        conv_data = self.relu(conv_data)#Second activation layer\n",
    "        conv_data = self.pool(conv_data)#Second pooling layer\n",
    "\n",
    "        \n",
    "        # Concatenate the additional features after flattening the output from convolutional layers\n",
    "        conv_data = self.flatten(conv_data)\n",
    "    \n",
    "        #Include option in forward pass to use the additional features or to not\n",
    "        if self.use_aa_features:\n",
    "            #print(conv_data.shape, ' ---- shape of conv data')\n",
    "\n",
    "            #print(additional_features.shape, ' ---- this is the shape of the additional features')\n",
    "            conv_data = torch.cat((conv_data, additional_features), dim=1)\n",
    "        else:\n",
    "            #print(conv_data.shape, ' --- shape of convolutions after pooling')\n",
    "            pass\n",
    "\n",
    "        #Apply linear layer 1\n",
    "        conv_data = self.Lin1(conv_data)\n",
    "\n",
    "        #Introduce non-linearity\n",
    "        conv_data = self.relu(conv_data)\n",
    "\n",
    "        #Apply Linear layer 2\n",
    "        conv_data = self.Lin2(conv_data)\n",
    "\n",
    "        #Squeeze the output\n",
    "        conv_data = conv_data.squeeze()\n",
    "\n",
    "        return conv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "sequence_length = encoded_sequence_length #dynamically determined\n",
    "num_additional_features = len(aa_values)#dynamically determined\n",
    "use_aa_features = True\n",
    "use_embedding_layer = True\n",
    "\n",
    "\n",
    "#User determined, empirically determined\n",
    "\n",
    "\n",
    "embedding_dimension = 64\n",
    "conv_layer_kernel_size = 3\n",
    "pooling_kernel_size = 2\n",
    "pool_stride = 2\n",
    "conv_stride = 1\n",
    "conv_padding = 1\n",
    "conv_dilation = 1\n",
    "\n",
    "lr_rate = 0.001 \n",
    "\n",
    "\n",
    "model = ProteinCNN(sequence_length = sequence_length,\n",
    "                    num_additional_features = num_additional_features,\n",
    "                    embedding_dim = embedding_dimension,\n",
    "                    conv_layer_kernel_size = conv_layer_kernel_size,\n",
    "                    pooling_kernel_size = pooling_kernel_size,\n",
    "                    pool_stride = pool_stride,\n",
    "                    conv_dilation = conv_dilation,\n",
    "                    conv_stride = conv_stride,\n",
    "                    use_aa_features = use_aa_features,\n",
    "                    use_embedding_layer = use_embedding_layer)\n",
    "\n",
    "loss_measurement = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Move model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function to calculate loss on validation dataset\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_batch_val_loss = 0\n",
    "    batch_val_samples = 0\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for seqs, features, labels in val_loader:\n",
    "            seqs = seqs.to(device)\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #Update sequence with transpose to make compatible with CNN architecture\n",
    "            \n",
    "            outputs = model(conv_data = seqs,\n",
    "                            additional_features = features\n",
    "                            )\n",
    "\n",
    "            loss = loss_measurement(outputs, labels)\n",
    "            all_batch_val_loss += loss.item() * seqs.size(0)\n",
    "            batch_val_samples += seqs.size(0)\n",
    "\n",
    "            all_val_preds.append(outputs.detach())\n",
    "            all_val_labels.append(labels)\n",
    "\n",
    "    average_validation_loss = all_batch_val_loss  / batch_val_samples\n",
    "    return average_validation_loss, all_val_preds, all_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop w/ valildation function. \n",
    "#We track performance across all epochs per training_data_slice/permutation. This way we can observe how performance changes with increased epochs\n",
    "\n",
    "#Store historical performance:\n",
    "history_across_all_training_slices = {}\n",
    "\n",
    "num_epochs = 10\n",
    "use_embedding_layer = True\n",
    "use_aa_features = False\n",
    "\n",
    "for pos, items in enumerate(training_data_slices.items()):\n",
    "    k = items[0]\n",
    "    v = items[1]\n",
    "    print(k, ' ---- Training data slice for ')\n",
    "   \n",
    "    history = {\n",
    "                'train_loss': [],\n",
    "                'val_loss': [],\n",
    "                'train_r2':[],\n",
    "                'val_r2':[],\n",
    "                'train_mae':[],\n",
    "                'val_mae':[],\n",
    "                'train_rmse':[],\n",
    "                'val_rmse':[]\n",
    "            }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            print('We are at epoch: ' + str(epoch))\n",
    "\n",
    "        model.train()\n",
    "        all_batch_train_loss = 0\n",
    "        batch_samples = 0\n",
    "\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        for batch, (seqs, features, labels) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "            seqs = seqs.to(device)\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(conv_data = seqs,\n",
    "                            additional_features = features)  # Adjust dimensions for Conv1D\n",
    "            \n",
    "            all_train_preds.append(outputs.detach())\n",
    "            all_train_labels.append(labels)\n",
    "\n",
    "            loss = loss_measurement(outputs, labels)\n",
    "\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(loss, ' ---- this is the loss')\n",
    "            #Add loss per batch and normalize per batch size\n",
    "            all_batch_train_loss += loss.item() * seqs.size(0)\n",
    "            batch_samples += seqs.size(0)\n",
    "\n",
    "        avg_batch_training_loss = all_batch_train_loss / batch_samples\n",
    "        #print(avg_batch_training_loss)\n",
    "        \n",
    "        # Validate after each epoch\n",
    "        avg_validation_loss, epoch_val_preds, epoch_val_labels = validate(model, val_loader)\n",
    "\n",
    "        # Converting lists to tensors\n",
    "        train_preds = torch.cat(all_train_preds)\n",
    "        train_labels = torch.cat(all_train_labels)\n",
    "        val_preds = torch.cat(epoch_val_preds)\n",
    "        val_labels = torch.cat(epoch_val_labels)\n",
    "\n",
    "        # Calculate metrics for training\n",
    "        history['train_r2'].append(r2_score(train_labels.numpy(), train_preds.numpy()))\n",
    "        history['train_mae'].append(mean_absolute_error(train_labels.numpy(), train_preds.numpy()))\n",
    "        history['train_rmse'].append(mean_squared_error(train_labels.numpy(), train_preds.numpy(), squared=False))\n",
    "        \n",
    "        # Calculate metrics for validation\n",
    "        history['val_r2'].append(r2_score(val_labels.numpy(), val_preds.numpy()))\n",
    "        history['val_mae'].append(mean_absolute_error(val_labels.numpy(), val_preds.numpy()))\n",
    "        history['val_rmse'].append(mean_squared_error(val_labels.numpy(), val_preds.numpy(), squared=False))\n",
    "\n",
    "        #Store the loss\n",
    "        history['train_loss'].append(avg_batch_training_loss)\n",
    "        history['val_loss'].append(avg_validation_loss)\n",
    "\n",
    "        history_across_all_training_slices[k] = history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation time: Lets see some plots\n",
    "- Ideally we wantto see all the training slices compared against each other on the same plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming num_epochs and history_across_all_training_slices are defined\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "metrics = ['r2', 'mae', 'rmse', 'loss']\n",
    "titles = ['R2 Score per Epoch', 'MAE per Epoch', 'RMSE per Epoch', 'Loss per Epoch']\n",
    "y_labels = ['R2 Score', 'Mean Absolute Error', 'Root Mean Square Error', 'Loss']\n",
    "\n",
    "# Create a figure object to manage overall plot properties\n",
    "fig, axs = plt.subplots(1, 4, figsize=(25, 5))  # Adjust the figure size based on your display/requirements\n",
    "\n",
    "# Collect handles and labels for the legend\n",
    "handles, labels = [], []\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axs[i]  # Use subplot axis for plotting\n",
    "    for slice_key, history in history_across_all_training_slices.items():\n",
    "        train_metric = history[f'train_{metric}']\n",
    "        val_metric = history[f'val_{metric}']\n",
    "        handle1, = ax.plot(epochs, train_metric, label=f'Train {metric.upper()} - {slice_key}')\n",
    "        handle2, = ax.plot(epochs, val_metric, label=f'Val {metric.upper()} - {slice_key}', linestyle='--')\n",
    "\n",
    "        # Add handles for the first iteration to avoid duplicates in the legend\n",
    "        if i == 0:\n",
    "            handles.extend([handle1, handle2])\n",
    "            labels.extend([f'Train {metric.upper()} - {slice_key}', f'Val {metric.upper()} - {slice_key}'])\n",
    "\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(y_labels[i])\n",
    "\n",
    "# Place a single legend on the right side of the figure\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs should be consistent across all slices if your training regimen does not change\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Create a separate figure for each metric to compare across slices\n",
    "metrics = ['r2', 'mae', 'rmse', 'loss']\n",
    "titles = ['R2 Score per Epoch', 'MAE per Epoch', 'RMSE per Epoch', 'Loss per Epoch']\n",
    "y_labels = ['R2 Score', 'Mean Absolute Error', 'Root Mean Square Error', 'Loss']\n",
    "\n",
    "plt.figure(figsize=(20, 10))  # Adjust the figure size based on your display/requirements\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 4, i + 1) # 4 subplots for 4 metrics\n",
    "\n",
    "    for slice_key, history in history_across_all_training_slices.items():\n",
    "        train_metric = history[f'train_{metric}']\n",
    "        val_metric = history[f'val_{metric}']\n",
    "        plt.plot(epochs, train_metric, label=f'Train {metric.upper()} - {slice_key}')\n",
    "        plt.plot(epochs, val_metric, label=f'Val {metric.upper()} - {slice_key}', linestyle='--')\n",
    "\n",
    "    plt.title(titles[i])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(y_labels[i])\n",
    "    plt.legend(loc='best')  # dynamically adjust the legend location\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual training slice visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Example of plotting R-squared scores for training and validation\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(epochs, history['train_r2'], label='Train R2')\n",
    "plt.plot(epochs, history['val_r2'], label='Validation R2')\n",
    "plt.title('R2 Score per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting MAE and RMSE similarly\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(epochs, history['train_mae'], label='Train MAE')\n",
    "plt.plot(epochs, history['val_mae'], label='Validation MAE')\n",
    "plt.title('MAE per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(epochs, history['train_rmse'], label='Train RMSE')\n",
    "plt.plot(epochs, history['val_rmse'], label='Validation RMSE')\n",
    "plt.title('RMSE per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Mean Square Error')\n",
    "plt.legend()\n",
    "\n",
    "#Let plot the loss\n",
    "plt.subplot(1,4,4)\n",
    "plt.plot(epochs, history['train_loss'], label = 'Training Loss')\n",
    "plt.plot(epochs, history['val_loss'], label = 'Validation Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a-alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
